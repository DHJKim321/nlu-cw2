BASELINE
==============================================================================================================================
INFO: Epoch 099: loss 2.138 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 29.27 | clip 1
INFO: Epoch 099: valid_loss 3.3 | num_tokens 13.8 | batch_size 500 | valid_perplexity 27.2
BLEU = 10.86, 40.8/14.2/6.7/3.6 (BP=1.000, ratio=1.002, hyp_len=6306, ref_len=6295)

Encoder layers = 2, Decoder layers = 3
==============================================================================================================================
INFO: Epoch 092: loss 2.378 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 23.54 | clip 1
INFO: Epoch 092: valid_loss 3.37 | num_tokens 13.8 | batch_size 500 | valid_perplexity 29
BLEU = 10.05, 40.0/13.2/6.2/3.3 (BP=0.988, ratio=0.988, hyp_len=6221, ref_len=6295)

Lexical Encoding
==============================================================================================================================
INFO: Epoch 046: loss 1.916 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 29.03 | clip 0.998
INFO: Epoch 046: valid_loss 3.19 | num_tokens 13.8 | batch_size 500 | valid_perplexity 24.3
BLEU = 13.06, 43.7/17.1/8.6/4.8 (BP=0.983, ratio=0.983, hyp_len=6190, ref_len=6295)